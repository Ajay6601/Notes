**ðŸŽ¯ Purpose:** Master RAG concepts to crack any AI/ML interview

**ðŸ“š What You'll Learn:**
- Why RAG exists and what problems it solves
- Different RAG approaches (when to use what)
- GraphRAG vs Vector RAG (the key distinction)
- How to answer any RAG interview question
- Production trade-offs and real-world decisions

---

## Table of Contents
1. [The Big Picture: Why RAG Matters](#big-picture)
2. [RAG Fundamentals (Start Here)](#rag-fundamentals)
3. [Vector RAG - The Standard Approach](#vector-rag)
4. [Knowledge Graphs - Adding Structure](#knowledge-graphs)
5. [GraphRAG - When You Need Multi-Hop Reasoning](#graphrag)
6. [Hybrid RAG - Combining Approaches](#hybrid-rag)
7. [Advanced Patterns (Interview Gold)](#advanced-rag)
8. [Databases: Choosing the Right One](#graph-databases)
9. [The Decision Matrix (Memorize This)](#tradeoffs)
10. [Interview Questions You'll Actually Get Asked](#interview-questions)

---

## ðŸŽ¯ The Big Picture: Why RAG Matters {#big-picture}

### The Core Problem RAG Solves

**Imagine you're interviewing at OpenAI, Anthropic, or any AI company. They ask:**
> "Why can't we just use GPT-4 directly? Why do we need RAG?"

**Your answer should hit these 4 points:**

**1. Knowledge Cutoff**
- GPT-4 trained on data until October 2023
- User asks: "What happened in the 2024 Olympics?"
- LLM: âŒ "I don't have information about future events"
- RAG: âœ… Retrieves recent news articles â†’ Answers correctly

**2. Hallucination**
- LLM generates plausible but **false** information
- Example: "Company X's Q3 revenue was $50M" (made up)
- RAG: âœ… Retrieves actual financial report â†’ Cites source

**3. Private/Proprietary Data**
- LLM can't access your company's internal documents
- Example: "What's our return policy for enterprise customers?"
- RAG: âœ… Retrieves from company knowledge base

**4. Cost of Retraining**
- Retraining GPT-4 costs millions of dollars
- Takes weeks/months
- RAG: âœ… Just update document database (costs pennies)

### The One-Sentence Explanation

**For interviews, memorize this:**
> "RAG is like giving an LLM a library card - instead of relying on memorized knowledge, it can look up current, accurate information before answering."

---

## 1. RAG Fundamentals (Start Here) {#rag-fundamentals}

### How RAG Actually Works (The 5-Step Process)

Think of RAG like asking a smart librarian a question:

**Step 1: User asks a question**
```
User: "What is our company's remote work policy?"
```

**Step 2: Convert question to searchable format (Embedding)**
- Computer can't search text directly
- Converts question to numbers: `[0.2, 0.8, 0.1, ...]` (1536 numbers)
- This is called an "embedding" or "vector"

**Step 3: Search the database (Retrieval)**
- Compare question embedding to document embeddings
- Find most similar documents (like "Ctrl+F" but smarter)
- Retrieve top 5-10 most relevant chunks

**Step 4: Give context to LLM (Augmentation)**
- Take retrieved documents
- Create prompt: "Based on these documents: [docs], answer: [question]"
- This is the "augmentation" part

**Step 5: LLM generates answer (Generation)**
- LLM reads the context
- Generates answer based on actual documents
- Can cite sources: "According to HR Policy Doc, page 5..."

### Why This Matters

**Without RAG:**
```
User: "What's our remote work policy?"
LLM: "I don't have access to your specific company policies."
```

**With RAG:**
```
User: "What's our remote work policy?"
System: [Retrieves HR policy document]
LLM: "According to your company's HR policy (updated Jan 2024), 
     employees can work remotely 3 days per week..."
```

### The Evolution: Three Generations

**Generation 1: Naive RAG (2020-2021)**
- Simple vector search
- Problem: Often retrieves irrelevant chunks
- Like searching with just keywords

**Generation 2: Advanced RAG (2022-2023)**
- Added reranking (double-check relevance)
- Hybrid search (keywords + semantic)
- Better, but still limited

**Generation 3: Modular RAG (2023-2024)**
- GraphRAG (understand relationships)
- Self-RAG (LLM critiques itself)
- Agentic RAG (multi-step reasoning)
- This is where the field is now

### Key Insight for Interviews

**Interviewer will ask: "What's the hardest part of building RAG?"**

**Your answer:**
> "The retrieval quality. You can have the best LLM, but if you retrieve irrelevant documents, the answer will be wrong. It's the 'garbage in, garbage out' problem. That's why we need good chunking strategies, hybrid search, and reranking."

---

## 1. RAG Fundamentals & Evolution {#rag-fundamentals}

### What is RAG?

**Retrieval-Augmented Generation (RAG)** combines retrieval systems with generative models to ground LLM responses in external knowledge.

**Core Problem RAG Solves:**
- LLMs have **knowledge cutoff dates** (training data ends at a specific point)
- LLMs **hallucinate** (generate plausible but incorrect information)
- LLMs can't access **private/proprietary data** (your company docs, databases)
- LLMs are **expensive to retrain** for new information

**RAG Solution:**
1. **Retrieve** relevant documents from external knowledge base
2. **Augment** LLM prompt with retrieved context
3. **Generate** response grounded in retrieved information

### Evolution of RAG Approaches

```
Generation 1: Naive RAG (2020-2021)
â”œâ”€ Vector embeddings + cosine similarity
â”œâ”€ Simple top-k retrieval
â””â”€ Direct context injection

Generation 2: Advanced RAG (2022-2023)
â”œâ”€ Query transformation
â”œâ”€ Reranking
â”œâ”€ Hybrid search (dense + sparse)
â””â”€ Multi-query strategies

Generation 3: Modular RAG (2023-2024)
â”œâ”€ GraphRAG (Microsoft)
â”œâ”€ Self-RAG
â”œâ”€ CRAG (Corrective RAG)
â”œâ”€ Agentic RAG
â””â”€ Multi-modal RAG
```

**Sources:**
- Lewis et al. (2020) - "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- Gao et al. (2023) - "Retrieval-Augmented Generation for Large Language Models: A Survey"
- Microsoft GraphRAG paper (2024)

---

## 2. Vector RAG (Standard/Naive RAG) {#vector-rag}

### Architecture

```
Documents â†’ Chunking â†’ Embedding â†’ Vector DB â†’ Retrieval â†’ LLM
```

**Step-by-step Process:**

1. **Document Processing**
   - Split documents into chunks (512-1024 tokens typical)
   - Preserve semantic coherence
   - Add metadata (source, timestamp, author)

2. **Embedding Generation**
   - Use embedding model (text-embedding-3-large, BGE, E5)
   - Convert chunks to dense vectors (768-3072 dimensions)
   - Store in vector database

3. **Query Processing**
   - Embed user query with same model
   - Compute similarity (cosine, dot product, L2)
   - Retrieve top-k chunks (k=3-10 typical)

4. **Context Augmentation**
   - Inject retrieved chunks into prompt
   - Format: "Given context: {chunks}, answer: {query}"
   - Send to LLM

5. **Generation**
   - LLM generates response
   - (Optional) Citation/source attribution

### Chunking Strategies

**Fixed-Size Chunking**
```python
def fixed_chunk(text, chunk_size=512, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i:i + chunk_size])
    return chunks
```
- **Pros:** Simple, fast, predictable
- **Cons:** Breaks semantic units, arbitrary splits

**Sentence-Based Chunking**
```python
def sentence_chunk(text, max_sentences=10):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current = []
    for sent in sentences:
        current.append(sent)
        if len(current) >= max_sentences:
            chunks.append(' '.join(current))
            current = []
    return chunks
```
- **Pros:** Preserves sentence boundaries
- **Cons:** Variable chunk sizes, may break context

**Semantic Chunking** (Advanced)
```python
def semantic_chunk(text, embedding_model, threshold=0.7):
    sentences = nltk.sent_tokenize(text)
    embeddings = [embedding_model.encode(s) for s in sentences]
    
    chunks = []
    current = [sentences[0]]
    
    for i in range(1, len(sentences)):
        similarity = cosine_similarity(embeddings[i-1], embeddings[i])
        if similarity < threshold:  # Topic shift detected
            chunks.append(' '.join(current))
            current = [sentences[i]]
        else:
            current.append(sentences[i])
    return chunks
```
- **Pros:** Respects semantic boundaries
- **Cons:** Computationally expensive, requires tuning

**Recursive Chunking** (LangChain approach)
- Split by paragraph â†’ sentence â†’ character
- Preserves hierarchy
- Best for structured documents

**Late Chunking** (Jina AI, 2024)
- Embed full document first
- Then split embeddings
- Preserves global context

### Embedding Models Comparison

| Model | Dimensions | MTEB Score | Speed | Use Case |
|-------|-----------|------------|-------|----------|
| text-embedding-3-small | 1536 | 62.3 | Fast | General purpose |
| text-embedding-3-large | 3072 | 64.6 | Medium | High accuracy |
| BGE-large-en-v1.5 | 1024 | 63.9 | Fast | Open-source leader |
| E5-mistral-7b-instruct | 4096 | 66.6 | Slow | SOTA (requires GPU) |
| Cohere embed-v3 | 1024 | 64.5 | Fast | Multilingual |
| voyage-02 | 1024 | 65.1 | Fast | Domain-specific |

**Key Considerations:**
- **Dimension size:** Higher â‰  always better (diminishing returns, cost)
- **Domain adaptation:** Fine-tune on your domain for +5-10% accuracy
- **Multilingual:** mE5, multilingual-e5 for non-English
- **Cost:** OpenAI charges per token, open-source models free (hosting cost)

### Vector Databases Deep Dive

**Architecture Requirements:**
1. **Indexing:** Fast insertion and updates
2. **Search:** Low-latency similarity search (<50ms)
3. **Filtering:** Metadata filtering (date, source, category)
4. **Scalability:** Millions to billions of vectors

**Popular Vector DBs:**

**Pinecone** (Managed Cloud)
```python
import pinecone

pinecone.init(api_key="xxx", environment="us-west1-gcp")
index = pinecone.Index("my-index")

# Upsert vectors
index.upsert(vectors=[
    ("id1", [0.1, 0.2, ...], {"source": "doc1.pdf"}),
    ("id2", [0.3, 0.4, ...], {"source": "doc2.pdf"}),
])

# Query
results = index.query(
    vector=[0.15, 0.25, ...],
    top_k=5,
    filter={"source": "doc1.pdf"}
)
```
- **Pros:** Fully managed, scales automatically, great DX
- **Cons:** Expensive ($70+/month), vendor lock-in
- **Use case:** Startups, prototyping

**Qdrant** (Rust-based, Fast)
```python
from qdrant_client import QdrantClient

client = QdrantClient(host="localhost", port=6333)

client.upsert(
    collection_name="my_collection",
    points=[
        PointStruct(id=1, vector=[0.1, 0.2], payload={"text": "..."})
    ]
)

results = client.search(
    collection_name="my_collection",
    query_vector=[0.15, 0.25],
    limit=5
)
```
- **Pros:** Fast (Rust), open-source, good filtering
- **Cons:** Smaller community, self-hosted complexity
- **Use case:** High-performance requirements

**Weaviate** (Hybrid Search)
```python
import weaviate

client = weaviate.Client("http://localhost:8080")

# Hybrid search (vector + keyword)
results = client.query.get("Document", ["text"]).with_hybrid(
    query="What is RAG?",
    alpha=0.75  # 0.75 = 75% vector, 25% keyword
).with_limit(5).do()
```
- **Pros:** Native hybrid search, GraphQL API, good docs
- **Cons:** Complex setup, resource-intensive
- **Use case:** Hybrid retrieval needs

**ChromaDB** (Simple, Local)
```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("my_docs")

collection.add(
    documents=["This is a document", "Another document"],
    ids=["id1", "id2"]
)

results = collection.query(
    query_texts=["What is this about?"],
    n_results=5
)
```
- **Pros:** Simplest API, great for prototyping, free
- **Cons:** Not for production scale, limited features
- **Use case:** Local development, demos

**Milvus** (Cloud-Native, Scalable)
```python
from pymilvus import Collection

collection = Collection("my_collection")
collection.insert([
    [1, 2, 3],  # IDs
    [[0.1, 0.2], [0.3, 0.4]]  # Vectors
])

results = collection.search(
    data=[[0.15, 0.25]],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=5
)
```
- **Pros:** Kubernetes-native, highly scalable, GPU support
- **Cons:** Complex deployment, steep learning curve
- **Use case:** Enterprise scale (100M+ vectors)

**pgvector** (PostgreSQL Extension)
```sql
CREATE EXTENSION vector;

CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    embedding VECTOR(1536),
    text TEXT
);

CREATE INDEX ON documents USING ivfflat (embedding vector_l2_ops);

SELECT id, text, embedding <-> '[0.1,0.2,...]' AS distance
FROM documents
ORDER BY distance
LIMIT 5;
```
- **Pros:** Existing Postgres infrastructure, ACID guarantees
- **Cons:** Slower than specialized DBs, limited to ~1M vectors
- **Use case:** Small-medium datasets, existing Postgres users

### Index Types Explained

**HNSW (Hierarchical Navigable Small World)**
- Graph-based index
- Search complexity: O(log n)
- Build time: O(n log n)
- **Best for:** High recall (95%+), read-heavy workloads
- **Used by:** Qdrant, Weaviate, Pinecone

**IVF (Inverted File Index)**
- Clustering-based (k-means)
- Search complexity: O(n/k) where k = clusters
- **Best for:** Large datasets, acceptable recall (85-90%)
- **Used by:** FAISS, Milvus

**Product Quantization (PQ)**
- Compression technique
- Reduces memory 8-32x
- **Best for:** Billion-scale datasets
- **Trade-off:** Lower accuracy (~5-10% recall drop)

**Comparison:**

| Index | Build Time | Query Time | Memory | Recall |
|-------|-----------|------------|--------|--------|
| Flat (brute force) | O(1) | O(n) | High | 100% |
| IVF-Flat | O(n log n) | O(n/k) | High | 90-95% |
| IVF-PQ | O(n log n) | O(n/k) | Low | 85-90% |
| HNSW | O(n log n) | O(log n) | High | 95-99% |

**Rule of thumb:**
- <1M vectors: HNSW (prioritize recall)
- 1M-100M vectors: IVF-Flat or HNSW
- 100M+ vectors: IVF-PQ (memory constraints)

### Limitations of Vector RAG

1. **Lost in the Middle**
   - LLMs attend less to middle of context
   - Solution: Rerank, put important info at start/end

2. **Retrieval Failures**
   - Query-document mismatch (semantic gap)
   - Solution: Query transformation, HyDE

3. **Context Window Limits**
   - Can only fit 5-10 chunks (8K context)
   - Solution: Reranking, hierarchical retrieval

4. **No Reasoning Over Multiple Hops**
   - Can't connect info across documents
   - **This is where GraphRAG excels**

5. **Entity/Relationship Blindness**
   - Doesn't understand "who knows whom"
   - Doesn't capture graph structure
   - **This is where Knowledge Graphs excel**

---

## 3. Knowledge Graphs Deep Dive {#knowledge-graphs}

### What is a Knowledge Graph?

A **Knowledge Graph (KG)** is a structured representation of knowledge as entities and relationships:

```
(Entity) --[Relationship]--> (Entity)
```

**Example:**
```
(Elon Musk) --[CEO_OF]--> (Tesla)
(Tesla) --[MANUFACTURES]--> (Electric Vehicles)
(Electric Vehicles) --[REDUCE]--> (Carbon Emissions)
```

### RDF Triples (Subject-Predicate-Object)

The fundamental unit of a KG is a **triple**:

```
(Subject, Predicate, Object)
```

**Examples:**
```
(Paris, capital_of, France)
(France, located_in, Europe)
(Europe, has_population, 746_million)
```

**Why Triples?**
- Machine-readable format
- Enables logical reasoning
- Standardized (RDF, OWL)
- Query with SPARQL

### Knowledge Graph vs. Vector Database

| Feature | Vector DB | Knowledge Graph |
|---------|-----------|----------------|
| **Structure** | Flat (vectors) | Graph (entities + edges) |
| **Query** | Similarity | Traversal, reasoning |
| **Relationships** | Implicit (embeddings) | Explicit (edges) |
| **Reasoning** | No | Yes (multi-hop) |
| **Explainability** | Low | High |
| **Use case** | Semantic search | Complex reasoning |

**Example Query:**

**Vector DB:**
```
Query: "Who are Tesla's competitors?"
Returns: Documents mentioning Tesla and other car companies
```

**Knowledge Graph:**
```
MATCH (tesla:Company {name: "Tesla"})
MATCH (tesla)-[:COMPETES_WITH]->(competitor)
RETURN competitor.name
```
Returns: Explicit competitor relationships

**Multi-hop reasoning (KG advantage):**
```
MATCH (elon:Person {name: "Elon Musk"})
      -[:CEO_OF]->(company)
      -[:MANUFACTURES]->(product)
      -[:USES]->(technology)
RETURN technology
```
Finds: Technologies used in products of companies Elon leads

### Building Knowledge Graphs

**Manual Curation (Traditional)**
- Domain experts create ontology
- Expensive, time-consuming
- High quality
- Examples: Wikidata, DBpedia

**Automated Extraction from Text**

**Named Entity Recognition (NER)**
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Elon Musk founded SpaceX in 2002.")

entities = [(ent.text, ent.label_) for ent in doc.ents]
# [("Elon Musk", "PERSON"), ("SpaceX", "ORG"), ("2002", "DATE")]
```

**Relation Extraction**
```python
# Using LLM for relation extraction
prompt = """
Extract relations from: "Elon Musk founded SpaceX in 2002."

Format: (subject, relation, object)
"""

response = llm.generate(prompt)
# Output: (Elon Musk, FOUNDED, SpaceX)
#         (SpaceX, FOUNDED_IN, 2002)
```

**LLM-based KG Construction** (2024 approach)
```python
from langchain.graphs import Neo4jGraph

graph = Neo4jGraph(url="bolt://localhost:7687")

# Extract entities and relations with GPT-4
prompt = f"""
From this text, extract all entities and relationships:
{document}

Return as JSON:
{{
  "entities": [
    {{"name": "...", "type": "PERSON|ORG|LOCATION|..."}}
  ],
  "relationships": [
    {{"source": "...", "relation": "...", "target": "..."}}
  ]
}}
"""

result = json.loads(llm.generate(prompt))

# Add to Neo4j
for entity in result["entities"]:
    graph.query(f"CREATE (n:{entity['type']} {{name: '{entity['name']}'}})")

for rel in result["relationships"]:
    graph.query(f"""
        MATCH (a {{name: '{rel['source']}'}})
        MATCH (b {{name: '{rel['target']}'}})
        CREATE (a)-[:{rel['relation']}]->(b)
    """)
```

### Ontologies and Schema Design

**Ontology** = Formal specification of concepts and relationships in a domain

**Example: Company Ontology**
```turtle
@prefix : <http://example.org/company#> .

:Company a owl:Class .
:Person a owl:Class .
:Product a owl:Class .

:founded_by a owl:ObjectProperty ;
    rdfs:domain :Company ;
    rdfs:range :Person .

:produces a owl:ObjectProperty ;
    rdfs:domain :Company ;
    rdfs:range :Product .
```

**Schema Design Principles:**
1. **Classes** (entity types): Person, Company, Product
2. **Properties** (relationships): founded_by, produces, located_in
3. **Constraints**: Domain, range, cardinality
4. **Hierarchy**: Subclasses (CEO âŠ‚ Person)

### Knowledge Graph Embedding

**Why Embed KGs?**
- Enable similarity search
- Integrate with neural models
- Learn representations from structure

**TransE (Translational Embedding)**
```
h + r â‰ˆ t

Where:
h = head entity vector
r = relation vector
t = tail entity vector
```

**Example:**
```
Paris + capital_of â‰ˆ France
```

**Training objective:**
Minimize: ||h + r - t||

**Other Methods:**
- **DistMult:** Bilinear scoring
- **ComplEx:** Complex-valued embeddings
- **RotatE:** Rotational transformations
- **ConvE:** Convolutional networks

**Use in RAG:**
```python
# Hybrid retrieval: Vector + KG embeddings
vector_score = cosine_similarity(query_emb, doc_emb)
kg_score = graph.get_path_score(query_entity, doc_entity)

final_score = 0.7 * vector_score + 0.3 * kg_score
```

---

## 4. GraphRAG Explained {#graphrag}

### What is GraphRAG?

**GraphRAG** (Microsoft, 2024) combines Knowledge Graphs with RAG to enable multi-hop reasoning and global understanding.

**Key Innovation:**
- Standard RAG: Retrieves chunks based on similarity
- GraphRAG: Builds KG from documents, retrieves via graph traversal

**Paper:** "From Local to Global: A Graph RAG Approach to Query-Focused Summarization" (Microsoft, 2024)

### Architecture Overview

```
Documents â†’ LLM Extraction â†’ Knowledge Graph â†’ Community Detection â†’ Summarization â†’ Query
```

**Detailed Pipeline:**

**Phase 1: Graph Construction**
1. **Entity/Relationship Extraction**
   - Use LLM to extract entities and relations from each chunk
   - Example: "Elon Musk founded Tesla" â†’ (Elon Musk, FOUNDED, Tesla)

2. **Graph Building**
   - Create nodes for entities
   - Create edges for relationships
   - Add document references

3. **Entity Resolution**
   - Merge duplicate entities ("Tesla" = "Tesla Inc." = "Tesla Motors")
   - Use fuzzy matching or LLM

**Phase 2: Community Detection**
```python
import networkx as nx
from cdlib import algorithms

G = nx.Graph()
# Add nodes and edges from KG
...

# Leiden algorithm for community detection
communities = algorithms.leiden(G)

# Result: Clusters of related entities
# Community 1: {Tesla, SpaceX, Elon Musk, Electric Vehicles}
# Community 2: {OpenAI, Sam Altman, GPT-4, ChatGPT}
```

**Why communities?**
- Group related information
- Enable hierarchical reasoning
- Improve retrieval relevance

**Phase 3: Community Summarization**
```python
# For each community, generate summary
for community in communities:
    entities = community.nodes
    relations = community.edges
    
    prompt = f"""
    Summarize this group of entities and their relationships:
    Entities: {entities}
    Relationships: {relations}
    
    Provide a concise summary of what this community represents.
    """
    
    summary = llm.generate(prompt)
    store_summary(community.id, summary)
```

**Phase 4: Query-Time Processing**

**Local Search (Entity-Focused)**
```python
def local_search(query, graph):
    # 1. Extract entities from query
    query_entities = extract_entities(query)
    
    # 2. Find related entities (1-2 hops)
    neighbors = graph.get_neighbors(query_entities, hops=2)
    
    # 3. Retrieve relevant chunks
    chunks = get_chunks_for_entities(neighbors)
    
    # 4. Generate answer
    return llm.generate(f"Context: {chunks}\n\nQuery: {query}")
```

**Global Search (Community-Focused)**
```python
def global_search(query, graph, communities):
    # 1. Rank communities by relevance
    community_scores = rank_communities(query, communities)
    
    # 2. Retrieve top-k community summaries
    top_summaries = [communities[i].summary 
                     for i in top_k(community_scores)]
    
    # 3. Generate answer from summaries
    return llm.generate(f"""
        Based on these topic summaries: {top_summaries}
        Answer: {query}
    """)
```

### GraphRAG vs. Standard RAG

**Comparison Table:**

| Feature | Standard RAG | GraphRAG |
|---------|-------------|----------|
| **Retrieval** | Similarity search | Graph traversal |
| **Context** | Independent chunks | Connected entities |
| **Reasoning** | Single-hop | Multi-hop |
| **Global queries** | Poor | Excellent |
| **Setup cost** | Low | High (KG building) |
| **Query latency** | 50-200ms | 200-1000ms |
| **Explainability** | Low | High (graph paths) |

**Example Queries:**

**Query 1: "What are the main themes in this dataset?"**
- Standard RAG: Retrieves random chunks, struggles to see big picture
- GraphRAG: Uses community summaries, identifies global themes

**Query 2: "How is X connected to Y?"**
- Standard RAG: Only finds if explicitly mentioned together
- GraphRAG: Traverses graph, finds indirect connections

**Query 3: "Who are the key people in this organization?"**
- Standard RAG: Returns documents mentioning people
- GraphRAG: Ranks by centrality in graph (degree, PageRank)

### Implementation Example

**Full GraphRAG Pipeline:**

```python
from graphrag import GraphRAG

# Initialize
graphrag = GraphRAG(
    llm="gpt-4",
    embedding_model="text-embedding-3-large",
    graph_db="neo4j://localhost:7687"
)

# Phase 1: Build graph from documents
documents = load_documents("./data")
graphrag.build_graph(documents)

# Phase 2: Detect communities
graphrag.detect_communities(algorithm="leiden")

# Phase 3: Generate summaries
graphrag.summarize_communities()

# Phase 4: Query
query = "What are the main research themes?"
response = graphrag.query(query, mode="global")  # or "local"

print(response.answer)
print(response.sources)  # Graph paths as citations
```

**Custom Graph Query:**

```python
# Query Neo4j directly for complex reasoning
def multi_hop_query(start_entity, relationship_chain):
    """
    Example: Find all technologies used by companies Elon Musk founded
    
    relationship_chain = ["FOUNDED", "MANUFACTURES", "USES"]
    """
    cypher = f"""
    MATCH path = (start {{name: '{start_entity}'}})
    {'-[:{r}]->()'.join([''] + relationship_chain + [''])}
    RETURN path
    """
    
    results = graph.query(cypher)
    return results
```

### Microsoft's GraphRAG Results (Paper)

**Datasets Tested:**
- Podcast transcripts
- News articles
- Scientific papers

**Metrics:**
- **Comprehensiveness:** GraphRAG >> Standard RAG (2-3x better)
- **Diversity:** GraphRAG covers more aspects
- **Empowerment:** Users find answers to complex questions

**Cost:**
- 10-20x more expensive (LLM calls for extraction, summarization)
- **Trade-off:** Quality vs. cost

**When to use:**
- Dataset: Large, interconnected documents
- Queries: Require multi-hop reasoning or global understanding
- Budget: Can afford higher cost for better quality

**When NOT to use:**
- Simple fact lookup (vector RAG sufficient)
- Small dataset (<1000 docs)
- Low budget (extraction + summarization expensive)

---

## 5. Hybrid RAG Approaches {#hybrid-rag}

### Dense + Sparse Retrieval

**Problem:** Dense embeddings miss exact keyword matches

**Solution:** Combine vector (dense) + BM25 (sparse)

**BM25 (Best Match 25)**
```
score(q, d) = Î£ IDF(q_i) * (f(q_i, d) * (k1 + 1)) / 
                             (f(q_i, d) + k1 * (1 - b + b * |d| / avgdl))

Where:
- f(q_i, d) = term frequency of q_i in document d
- IDF(q_i) = inverse document frequency
- k1 = term frequency saturation (1.2-2.0)
- b = length normalization (0.75)
- |d| = document length
- avgdl = average document length
```

**Why BM25?**
- Exact keyword matching
- Handles rare terms well
- Fast (inverted index)

**Hybrid Search Implementation:**

```python
def hybrid_search(query, k=10, alpha=0.7):
    """
    alpha: weight for dense search (0-1)
    1-alpha: weight for sparse search
    """
    # Dense retrieval
    query_emb = embedding_model.encode(query)
    dense_results = vector_db.search(query_emb, k=k*2)
    
    # Sparse retrieval (BM25)
    sparse_results = bm25_search(query, k=k*2)
    
    # Normalize scores to [0, 1]
    dense_scores = min_max_normalize([r.score for r in dense_results])
    sparse_scores = min_max_normalize([r.score for r in sparse_results])
    
    # Combine
    combined = {}
    for doc_id, score in dense_scores.items():
        combined[doc_id] = alpha * score
    
    for doc_id, score in sparse_scores.items():
        combined[doc_id] = combined.get(doc_id, 0) + (1 - alpha) * score
    
    # Return top-k
    return sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k]
```

**Score Normalization Methods:**

**Min-Max Normalization:**
```python
def min_max_normalize(scores):
    min_s, max_s = min(scores), max(scores)
    return [(s - min_s) / (max_s - min_s) for s in scores]
```

**Z-score Normalization:**
```python
def zscore_normalize(scores):
    mean_s = np.mean(scores)
    std_s = np.std(scores)
    return [(s - mean_s) / std_s for s in scores]
```

**Rank Fusion (Alternative to score fusion):**

**Reciprocal Rank Fusion (RRF):**
```python
def reciprocal_rank_fusion(rankings, k=60):
    """
    rankings: List of ranked document lists
    k: Constant (typically 60)
    """
    scores = {}
    for ranking in rankings:
        for rank, doc_id in enumerate(ranking):
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)
    
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

**Example:**
```
Dense ranking: [doc1, doc2, doc3]
Sparse ranking: [doc3, doc1, doc4]

RRF scores:
doc1: 1/(60+0+1) + 1/(60+1+1) = 0.0164 + 0.0161 = 0.0325
doc2: 1/(60+1+1) = 0.0161
doc3: 1/(60+2+1) + 1/(60+0+1) = 0.0159 + 0.0164 = 0.0323
doc4: 1/(60+2+1) = 0.0159

Final: [doc1, doc3, doc2, doc4]
```

**When to use which fusion:**
- **Score fusion (weighted):** When you trust score magnitudes
- **Rank fusion (RRF):** When scores not comparable, more robust

### Weaviate Hybrid Search

```python
import weaviate

client = weaviate.Client("http://localhost:8080")

results = client.query.get("Document", ["text", "source"]) \
    .with_hybrid(
        query="What is machine learning?",
        alpha=0.75,  # 0.75 = 75% vector, 25% BM25
        vector=query_embedding  # Optional: provide embedding
    ) \
    .with_limit(10) \
    .do()
```

**Alpha parameter tuning:**
```python
# Experiment to find best alpha
alphas = [0.0, 0.25, 0.5, 0.75, 1.0]
results = []

for alpha in alphas:
    recall = evaluate_recall(test_queries, alpha=alpha)
    results.append((alpha, recall))

best_alpha = max(results, key=lambda x: x[1])[0]
```

**Typical alpha values:**
- 0.0: Pure BM25 (keyword matching)
- 0.5: Balanced
- 0.75: Mostly vector (default)
- 1.0: Pure vector (semantic)

**When to adjust alpha:**
- **Lower alpha (more BM25):** Exact names, codes, IDs
- **Higher alpha (more vector):** Conceptual questions, paraphrasing

---

## 6. Advanced RAG Patterns {#advanced-rag}

### Query Transformation

**Problem:** User query doesn't match document phrasing

**Techniques:**

**1. Query Expansion**
```python
def expand_query(query, llm):
    prompt = f"""
    Generate 3 alternative phrasings of this query:
    "{query}"
    
    Return as JSON list.
    """
    
    alternatives = json.loads(llm.generate(prompt))
    return [query] + alternatives
```

**Example:**
```
Original: "How to fix Python errors?"
Expanded:
- "How to debug Python code?"
- "Troubleshooting Python exceptions"
- "Python error handling best practices"
```

**2. HyDE (Hypothetical Document Embeddings)**
```python
def hyde_retrieval(query, llm, vector_db):
    # Generate hypothetical answer
    prompt = f"""
    Write a detailed answer to: {query}
    (Even if you're not sure, write as if you know)
    """
    
    hypothetical_doc = llm.generate(prompt)
    
    # Embed and search
    hyp_embedding = embedding_model.encode(hypothetical_doc)
    results = vector_db.search(hyp_embedding, k=10)
    
    return results
```

**Why HyDE works:**
- Query is often short, vague
- Hypothetical answer is detailed, specific
- Better semantic match to actual documents

**Paper:** "Precise Zero-Shot Dense Retrieval without Relevance Labels" (Gao et al., 2022)

**3. Multi-Query Generation**
```python
def multi_query(query, llm, vector_db):
    # Generate multiple perspectives
    prompt = f"""
    Generate 3 diverse search queries related to: {query}
    Each should focus on a different aspect.
    """
    
    queries = json.loads(llm.generate(prompt))
    
    # Search with each
    all_results = []
    for q in queries:
        results = vector_db.search(embed(q), k=5)
        all_results.extend(results)
    
    # Deduplicate and rerank
    return deduplicate_and_rerank(all_results)
```

### Reranking

**Problem:** Initial retrieval (k=100) has good recall but many irrelevant results

**Solution:** Rerank with more expensive cross-encoder

**Two-Stage Retrieval:**
```
Stage 1: Bi-encoder (fast, k=100)
Stage 2: Cross-encoder (slow, k=10)
```

**Bi-encoder vs. Cross-encoder:**

**Bi-encoder (used in vector search):**
```python
query_emb = encoder(query)
doc_emb = encoder(doc)
score = cosine(query_emb, doc_emb)
```
- **Pro:** Precompute document embeddings, fast search
- **Con:** No query-doc interaction

**Cross-encoder (reranking):**
```python
score = encoder(concat(query, doc))
```
- **Pro:** Full attention between query and doc
- **Con:** Must compute for each query-doc pair (slow)

**Implementation:**
```python
from sentence_transformers import CrossEncoder

# Stage 1: Retrieve candidates
retriever = SentenceTransformer('all-MiniLM-L6-v2')
query_emb = retriever.encode(query)
candidates = vector_db.search(query_emb, k=100)

# Stage 2: Rerank
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
pairs = [[query, doc.text] for doc in candidates]
scores = reranker.predict(pairs)

# Sort by reranker scores
reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
top_k = reranked[:10]
```

**Popular Rerankers:**
- **ms-marco-MiniLM-L-6-v2:** Fast, good quality
- **bge-reranker-large:** SOTA, slower
- **Cohere rerank-english-v2.0:** API-based, excellent

**Cohere Rerank Example:**
```python
import cohere

co = cohere.Client(api_key="xxx")

results = co.rerank(
    query="What is RAG?",
    documents=[doc.text for doc in candidates],
    top_n=10,
    model="rerank-english-v2.0"
)

reranked_docs = [candidates[r.index] for r in results]
```

**When to rerank:**
- High recall needs (precision can be lower initially)
- Latency budget allows (adds 50-200ms)
- Complex queries (cross-encoder better at nuance)

### Self-RAG (Self-Reflective RAG)

**Paper:** "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection" (Asai et al., 2023)

**Key Idea:** LLM decides when to retrieve and critiques its own output

**Process:**
```
1. Generate initial answer (no retrieval)
2. LLM self-critique: "Is this answer correct? [Yes/No]"
3. If "No": Retrieve documents
4. Generate new answer with context
5. LLM self-critique: "Is this answer supported by context? [Yes/No]"
6. If "No": Retrieve more or refine
7. Return final answer
```

**Implementation:**
```python
def self_rag(query, llm, retriever):
    # Initial generation
    answer = llm.generate(query)
    
    # Self-critique
    critique_prompt = f"""
    Question: {query}
    Answer: {answer}
    
    Is this answer correct and complete? [Yes/No]
    """
    
    needs_retrieval = "No" in llm.generate(critique_prompt)
    
    if needs_retrieval:
        # Retrieve and regenerate
        docs = retriever.search(query, k=5)
        context = "\n".join([d.text for d in docs])
        
        answer = llm.generate(f"""
            Context: {context}
            Question: {query}
            Answer based on context:
        """)
        
        # Verify grounding
        verify_prompt = f"""
        Context: {context}
        Answer: {answer}
        
        Is the answer supported by the context? [Yes/No]
        """
        
        is_grounded = "Yes" in llm.generate(verify_prompt)
        
        if not is_grounded:
            # Try again or return with caveat
            answer = "Based on available information: " + answer
    
    return answer
```

**Benefits:**
- Reduces unnecessary retrieval (saves cost)
- Improves accuracy (self-verification)
- More interpretable (shows reasoning)

**Drawbacks:**
- More LLM calls (slower, costlier)
- Depends on LLM's self-critique accuracy

### CRAG (Corrective RAG)

**Paper:** "Corrective Retrieval Augmented Generation" (Yan et al., 2024)

**Key Idea:** Evaluate retrieved documents and take corrective actions

**Process:**
```
1. Retrieve documents
2. Grade each document: [Relevant / Ambiguous / Irrelevant]
3. Action based on grades:
   - All relevant: Proceed with generation
   - Some ambiguous: Web search for additional info
   - All irrelevant: Fallback to LLM knowledge or return "insufficient info"
```

**Relevance Grading:**
```python
def grade_document(query, doc, llm):
    prompt = f"""
    Query: {query}
    Document: {doc[:500]}
    
    Grade this document's relevance:
    A) Relevant - Contains answer to query
    B) Ambiguous - Partially relevant
    C) Irrelevant - Does not answer query
    
    Return only the letter.
    """
    
    grade = llm.generate(prompt, max_tokens=1)
    return grade
```

**CRAG Implementation:**
```python
def crag(query, retriever, llm, web_search):
    # Retrieve
    docs = retriever.search(query, k=5)
    
    # Grade
    grades = [grade_document(query, doc, llm) for doc in docs]
    
    # Analyze grades
    relevant = [d for d, g in zip(docs, grades) if g == 'A']
    ambiguous = [d for d, g in zip(docs, grades) if g == 'B']
    irrelevant = [d for d, g in zip(docs, grades) if g == 'C']
    
    # Corrective action
    if len(relevant) >= 3:
        # Sufficient relevant docs
        context = "\n".join([d.text for d in relevant])
    
    elif len(ambiguous) > 0:
        # Need more info
        web_results = web_search(query)
        context = "\n".join([d.text for d in relevant + ambiguous + web_results])
    
    else:
        # No relevant docs found
        return llm.generate(f"""
            {query}
            
            Note: No relevant documents found in knowledge base.
            Provide answer based on your training knowledge if possible,
            otherwise state that you don't have sufficient information.
        """)
    
    # Generate with context
    return llm.generate(f"Context: {context}\n\nQuery: {query}")
```

**Benefits:**
- Reduces hallucination (filters bad retrievals)
- Adaptive (uses web search when needed)
- More robust

**Drawbacks:**
- Complex pipeline (more failure points)
- Higher latency (grading step)

### Parent-Child Chunking

**Problem:** Small chunks lose context, large chunks dilute signal

**Solution:** Store small chunks for retrieval, large chunks (parents) for generation

**Architecture:**
```
Document
â”œâ”€â”€ Parent Chunk 1 (1000 tokens)
â”‚   â”œâ”€â”€ Child Chunk 1a (200 tokens) â† Embedded for search
â”‚   â”œâ”€â”€ Child Chunk 1b (200 tokens) â† Embedded for search
â”‚   â””â”€â”€ Child Chunk 1c (200 tokens) â† Embedded for search
â””â”€â”€ Parent Chunk 2 (1000 tokens)
    â”œâ”€â”€ Child Chunk 2a (200 tokens)
    â””â”€â”€ Child Chunk 2b (200 tokens)
```

**Implementation:**
```python
def create_parent_child_chunks(document, parent_size=1000, child_size=200):
    # Create parent chunks
    parents = []
    for i in range(0, len(document), parent_size):
        parent = document[i:i+parent_size]
        parents.append(parent)
    
    # Create children for each parent
    chunks = []
    for parent_id, parent in enumerate(parents):
        for j in range(0, len(parent), child_size):
            child = parent[j:j+child_size]
            chunks.append({
                'text': child,
                'parent_id': parent_id,
                'parent_text': parent
            })
    
    return chunks

# Retrieval
def retrieve_with_parents(query, chunks, k=5):
    # Search child chunks
    child_results = vector_search(query, chunks, k=k)
    
    # Fetch parent texts
    parent_texts = [chunk['parent_text'] for chunk in child_results]
    
    # Deduplicate parents
    unique_parents = list(set(parent_texts))
    
    return unique_parents
```

**Benefits:**
- Best of both worlds (precise retrieval + rich context)
- Reduces context pollution (vs. large chunks)

**Drawbacks:**
- Storage overhead (duplicate text in parents/children)
- Complexity in indexing

### Sentence Window Retrieval

**Variant:** Retrieve single sentences, expand to surrounding context

```python
def sentence_window_retrieval(query, k=5, window=3):
    # Embed individual sentences
    sentences = split_into_sentences(corpus)
    sentence_embeddings = [embed(s) for s in sentences]
    
    # Search
    top_sentences = search(query, sentence_embeddings, k=k)
    
    # Expand to window
    contexts = []
    for sent_idx in top_sentences:
        start = max(0, sent_idx - window)
        end = min(len(sentences), sent_idx + window + 1)
        context = ' '.join(sentences[start:end])
        contexts.append(context)
    
    return contexts
```

**When to use:**
- Precise factual lookup (dates, names)
- Large documents (want minimal context)

---

## 7. Graph Databases Comparison {#graph-databases}

### Neo4j (Most Popular)

**Cypher Query Language:**
```cypher
// Create nodes
CREATE (elon:Person {name: "Elon Musk"})
CREATE (tesla:Company {name: "Tesla"})
CREATE (elon)-[:CEO_OF]->(tesla)

// Query
MATCH (p:Person {name: "Elon Musk"})-[:CEO_OF]->(c:Company)
RETURN c.name

// Multi-hop
MATCH (p:Person)-[:CEO_OF]->(c:Company)-[:COMPETES_WITH]->(competitor)
RETURN competitor.name
```

**Python Integration:**
```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver("bolt://localhost:7687", 
                               auth=("neo4j", "password"))

with driver.session() as session:
    result = session.run("""
        MATCH (p:Person {name: $name})-[:CEO_OF]->(c:Company)
        RETURN c.name as company
    """, name="Elon Musk")
    
    for record in result:
        print(record["company"])
```

**Pros:**
- Mature, stable (20+ years)
- Rich ecosystem (plugins, visualization)
- ACID transactions
- Strong community

**Cons:**
- Commercial license for clustering (Enterprise)
- Memory-intensive
- Steeper learning curve

**Use case:** Complex graph traversals, fraud detection, social networks

### Amazon Neptune

**Supports two query languages:**
1. **Gremlin** (Apache TinkerPop)
2. **SPARQL** (RDF/semantic web)

**Gremlin Example:**
```python
from gremlin_python.driver import client

gremlin_client = client.Client('wss://your-neptune-endpoint:8182/gremlin')

query = """
g.V().has('name', 'Elon Musk')
     .out('CEO_OF')
     .values('name')
"""

results = gremlin_client.submit(query).all().result()
```

**SPARQL Example:**
```sparql
PREFIX : <http://example.org/>

SELECT ?company
WHERE {
    ?person :name "Elon Musk" .
    ?person :CEO_OF ?company .
}
```

**Pros:**
- Fully managed (AWS)
- Auto-scaling
- Multi-AZ replication
- Integrates with AWS services

**Cons:**
- Expensive
- AWS lock-in
- Less flexible than Neo4j

**Use case:** AWS-native architectures, need managed service

### Azure Cosmos DB (Gremlin API)

**Query:**
```python
from gremlin_python.driver import client

cosmos_client = client.Client(
    'wss://your-cosmos-account.gremlin.cosmos.azure.com:443/',
    'g',
    username="/dbs/graphdb/colls/graph",
    password="your-key"
)

query = "g.V().has('label', 'person').has('name', 'Elon Musk')"
results = cosmos_client.submit(query).all().result()
```

**Pros:**
- Multi-model (graph, document, key-value)
- Global distribution
- SLA guarantees (99.999%)

**Cons:**
- Expensive (RU-based pricing)
- Azure lock-in
- Gremlin implementation incomplete

**Use case:** Azure ecosystems, need multi-model

### TigerGraph

**GSQL (Graph SQL):**
```sql
CREATE QUERY find_competitors(STRING person_name) {
    Start = {Person.*};
    
    Result = SELECT c
             FROM Start:p -(CEO_OF)-> Company:comp 
                           -(COMPETES_WITH)-> Company:c
             WHERE p.name == person_name;
    
    PRINT Result;
}
```

**Pros:**
- Designed for analytics (OLAP)
- Real-time deep link analytics
- Fast (C++ core)
- Graph algorithms built-in (PageRank, shortest path, etc.)

**Cons:**
- Smaller community
- Complex setup
- Overkill for simple use cases

**Use case:** Large-scale graph analytics, recommendation engines

### Comparison Table

| Feature | Neo4j | Neptune | Cosmos DB | TigerGraph |
|---------|-------|---------|-----------|------------|
| **Query Language** | Cypher | Gremlin/SPARQL | Gremlin | GSQL |
| **Deployment** | Self-hosted/Cloud | AWS-managed | Azure-managed | Self-hosted/Cloud |
| **Scalability** | Moderate | High | High | Very High |
| **Cost** | Medium | High | High | Medium |
| **Use Case** | General | AWS apps | Azure apps | Analytics |
| **ACID** | Yes | Yes | Yes | Yes |
| **Graph Algorithms** | Plugin | Limited | Limited | Built-in |

### Choosing a Graph Database for RAG

**Decision Tree:**

```
Are you building RAG with KG?
â”‚
â”œâ”€ Yes, AWS-native? â†’ Amazon Neptune
â”œâ”€ Yes, Azure-native? â†’ Azure Cosmos DB (Gremlin)
â”œâ”€ Yes, analytics-heavy? â†’ TigerGraph
â”œâ”€ Yes, general purpose? â†’ Neo4j
â”‚
â””â”€ No, just vector search? â†’ Qdrant, Pinecone, Weaviate
```

**Recommendation for RAG:**
- **Start:** Neo4j (best docs, community, tooling)
- **Scale:** Consider managed (Neptune, Cosmos) if multi-region needed
- **Analytics:** TigerGraph if running graph algorithms (community detection, etc.)

---

## 8. Trade-offs Matrix {#tradeoffs}

### RAG Approach Comparison

| Approach | Setup Cost | Query Latency | Accuracy | Explainability | Best For |
|----------|-----------|---------------|----------|----------------|----------|
| **Vector RAG** | Low | 50-200ms | Good | Low | General Q&A |
| **Hybrid RAG** | Medium | 100-300ms | Better | Medium | Balanced needs |
| **GraphRAG** | High | 500-2000ms | Best (complex) | High | Multi-hop reasoning |
| **Self-RAG** | Low | 200-500ms | Better | High | Need verification |
| **CRAG** | Medium | 300-800ms | Better | High | Need robustness |

### Detailed Trade-off Analysis

**1. Accuracy vs. Latency**

```
High Accuracy (GraphRAG, CRAG)
â”‚
â”‚   âš ï¸ Trade-off: Slower queries (500-2000ms)
â”‚   ðŸ’° Trade-off: Higher cost (more LLM calls)
â”‚
Low Latency (Vector RAG)
    âš ï¸ Trade-off: Lower accuracy on complex queries
    ðŸ’° Benefit: Lower cost
```

**When to prioritize accuracy:**
- Medical/legal applications
- High-stakes decisions
- Budget allows

**When to prioritize latency:**
- Customer-facing chatbots
- Real-time applications
- Cost-sensitive

**2. Setup Cost vs. Maintenance**

```
High Setup (GraphRAG)
â”‚
â”‚   âœ… Benefit: Better long-term performance
â”‚   âš ï¸ Trade-off: Weeks of setup (KG building, tuning)
â”‚
Low Setup (Vector RAG)
    âœ… Benefit: Deploy in days
    âš ï¸ Trade-off: May need frequent tuning as queries evolve
```

**3. Explainability vs. Simplicity**

```
High Explainability (GraphRAG)
â”‚
â”‚   âœ… Can show: Entity â†’ Relation â†’ Entity paths
â”‚   âš ï¸ Trade-off: Complex pipeline, more to debug
â”‚
Low Explainability (Vector RAG)
    âœ… Simple pipeline
    âš ï¸ Hard to explain why a chunk was retrieved
```

### Cost Analysis

**Vector RAG (Baseline):**
```
Embedding: $0.0001 per 1K tokens
Storage: $0.10 per GB per month (Pinecone)
LLM: $0.03 per 1K tokens (GPT-4o-mini)

Example: 1000 queries/day
- Embedding: $0.10/day
- LLM: $30/day (assuming 1K tokens per query)
- Storage: $10/month (100K chunks)

Total: ~$950/month
```

**GraphRAG (Microsoft approach):**
```
KG Building (one-time):
- Entity extraction: $500-5000 (depends on corpus size)
- Community detection: $50 (compute)
- Summarization: $200-2000

Ongoing per query:
- Graph traversal: $0.001
- LLM generation: $0.03

Total: $2000-7000 upfront + $950/month
```

**When GraphRAG is worth it:**
- >10,000 queries/month
- Complex reasoning required
- Accuracy improvement justifies cost
- Long-term deployment (amortize setup cost)

### Chunking Strategy Trade-offs

| Strategy | Pros | Cons | Best For |
|----------|------|------|----------|
| **Fixed-size** | Simple, fast | Breaks semantics | Homogeneous docs |
| **Sentence-based** | Natural boundaries | Variable sizes | Factual Q&A |
| **Semantic** | Respects topics | Slow, complex | High-quality needs |
| **Recursive** | Preserves structure | Medium complexity | Structured docs |
| **Parent-child** | Best of both | Storage overhead | Mixed query types |

### Embedding Model Trade-offs

| Model | Dimensions | Speed | Accuracy | Cost | Best For |
|-------|-----------|-------|----------|------|----------|
| **text-embedding-3-small** | 1536 | Fast | Good | Low | High volume |
| **text-embedding-3-large** | 3072 | Medium | Better | Medium | Balanced |
| **E5-mistral-7b** | 4096 | Slow | Best | High | Max accuracy |
| **BGE-large** | 1024 | Fast | Good | Free (self-host) | Cost-sensitive |

### Vector DB Trade-offs

| Database | Setup | Scalability | Cost | Features | Best For |
|----------|-------|-------------|------|----------|----------|
| **Pinecone** | Easy | High | $$$ | Managed | Startups |
| **Qdrant** | Medium | High | $-$$ | Fast | Performance needs |
| **Weaviate** | Medium | High | $-$$ | Hybrid search | Balanced |
| **ChromaDB** | Easy | Low | Free | Simple | Prototyping |
| **Milvus** | Hard | Very High | $$-$$$ | Enterprise | Large scale |
| **pgvector** | Easy | Medium | $ | SQL integration | Existing Postgres |

**Decision Guide:**

```
Budget?
â”œâ”€ Tight â†’ ChromaDB (dev) or pgvector (prod)
â”œâ”€ Medium â†’ Qdrant or Weaviate (self-hosted)
â””â”€ High â†’ Pinecone (managed)

Scale?
â”œâ”€ <1M vectors â†’ pgvector, ChromaDB
â”œâ”€ 1M-100M â†’ Qdrant, Weaviate, Pinecone
â””â”€ >100M â†’ Milvus, Pinecone

Features needed?
â”œâ”€ Hybrid search â†’ Weaviate
â”œâ”€ Pure speed â†’ Qdrant
â”œâ”€ Existing Postgres â†’ pgvector
â””â”€ Managed service â†’ Pinecone
```

---

## 9. Interview Questions (Hao Hoang Style) {#interview-questions}

### Fundamentals & Concepts

**Q1: Explain RAG to a non-technical person. Then explain the key technical challenge.**

**Expected Answer:**
- **Non-technical:** "RAG is like giving an AI assistant a library card. Instead of only using what it memorized during training, it can look up current information in a database before answering your question."
  
- **Technical challenge:** The core problem is **retrieval quality**. How do you ensure the most relevant chunks are retrieved? This involves:
  - Query-document semantic mismatch (query is short, docs are long)
  - Lost-in-the-middle problem (LLM attends less to middle context)
  - Balancing precision (relevant) vs recall (comprehensive)

**Follow-up:** "How do you measure retrieval quality?"

**Answer:** 
- **Offline metrics:** Precision@k, Recall@k, NDCG, MAP
- **Online metrics:** Task success rate, user satisfaction, LLM answer quality (human eval or LLM-as-judge)

---

**Q2: You have 10,000 documents. A user asks a question. Walk through the entire RAG pipeline, including specific latency numbers at each step.**

**Expected Answer:**

```
1. Query Processing (5-10ms)
   - Embed query: text-embedding-3-large (1536 dims)
   - Optional: Query expansion/transformation

2. Retrieval (30-100ms)
   - Vector search: Top 100 candidates from Qdrant (30ms)
   - Optional: BM25 search in parallel (20ms)
   - Hybrid fusion: RRF or weighted combination (5ms)

3. Reranking (50-200ms)
   - Cross-encoder on top 100 â†’ top 10 (100ms)
   - Alternative: Cohere rerank API (80ms)

4. Context Construction (5-10ms)
   - Format retrieved chunks
   - Add metadata (sources, timestamps)

5. LLM Generation (500-3000ms)
   - Input tokens: 2000 (context) + 50 (query) = 2050
   - Output tokens: 500
   - GPT-4o: ~1500ms

Total: 590-3320ms (P50: ~2000ms, P95: ~3000ms)
```

**Optimization opportunities:**
- Cache embeddings for frequent queries (save step 1)
- Batch reranking (save 50ms)
- Streaming LLM output (perceived latency reduction)
- Faster model (GPT-4o-mini: 500ms instead of 1500ms)

**Follow-up:** "Latency budget is 500ms. What do you cut?"

**Answer:**
1. Skip reranking â†’ Lose 5-10% accuracy, save 100ms
2. Use smaller embedding model â†’ Lose 2-3% accuracy, save 5ms
3. Reduce candidates (100 â†’ 50) â†’ Lose 3% recall, save 15ms
4. **Use GPT-4o-mini instead of GPT-4o** â†’ Save 1000ms (biggest win)

New total: ~500ms achievable

---

**Q3: What's the difference between a vector database and a traditional database? When would you use each for RAG?**

**Expected Answer:**

**Traditional DB (Postgres):**
- Stores structured data (rows, columns)
- Queries: Exact matches, filters (WHERE, JOIN)
- Example: `SELECT * FROM docs WHERE category = 'AI'`
- **For RAG:** Use for metadata filtering, structured data

**Vector DB (Qdrant, Pinecone):**
- Stores embeddings (high-dimensional vectors)
- Queries: Similarity search (cosine, dot product)
- Example: "Find 10 most similar documents to query embedding"
- **For RAG:** Use for semantic retrieval

**Hybrid approach (Best for RAG):**
```python
# Step 1: Metadata filter in Postgres
candidates = db.query("""
    SELECT doc_id FROM documents 
    WHERE date > '2024-01-01' 
    AND category IN ('AI', 'ML')
""")

# Step 2: Semantic search in vector DB
results = vector_db.search(
    query_embedding,
    filter={"doc_id": candidates},  # Filter by Postgres results
    k=10
)
```

**When to use which:**
- **Small scale (<10K docs):** pgvector (Postgres extension) sufficient
- **Medium scale (10K-1M):** Dedicated vector DB (Qdrant, Weaviate)
- **Large scale (>1M):** Distributed vector DB (Milvus, Pinecone)
- **Need transactions/ACID:** Postgres + pgvector
- **Pure speed:** Qdrant (Rust-based)

---

### GraphRAG & Knowledge Graphs

**Q4: You're building a RAG system for a company's internal knowledge base (10,000 documents, lots of cross-references between people, projects, and teams). Should you use vector RAG or GraphRAG? Walk through your reasoning.**

**Expected Answer:**

**Analysis:**
- **Documents:** 10,000 (not huge, but significant)
- **Structure:** Interconnected (people â†” projects â†” teams)
- **Queries (likely):** 
  - "What projects is Alice working on?"
  - "Who else worked on Project X?"
  - "What's the connection between Team A and Team B?"

**Decision: GraphRAG (with caveats)**

**Reasoning:**

**Pros of GraphRAG:**
1. **Multi-hop queries:** "Who are the teammates of people Bob has collaborated with?" (2-hop)
2. **Entity-centric:** People, projects, teams are entities â†’ natural for KG
3. **Explainability:** Can show relationship paths (Alice â†’ Project X â†’ Bob)

**Cons of GraphRAG:**
1. **Setup cost:** 2-4 weeks to build KG (entity extraction, resolution, community detection)
2. **Maintenance:** Need to update KG as docs change
3. **Cost:** 10-20x more expensive than vector RAG (for KG construction)

**Hybrid Approach (Recommended):**
```python
def hybrid_company_rag(query):
    # Detect query type
    if is_entity_query(query):  # "Who is X?" "What projects...?"
        # Use GraphRAG
        entities = extract_entities(query)
        results = graph.traverse(entities, hops=2)
        return results
    else:  # "How do we do X?" "Best practices for Y?"
        # Use vector RAG
        results = vector_db.search(embed(query), k=10)
        return results
```

**Implementation:**
1. **Phase 1 (Week 1-2):** Deploy vector RAG (fast time-to-value)
2. **Phase 2 (Week 3-6):** Build KG in parallel
   - Extract entities (people, projects, teams) using LLM
   - Build relationships from co-mentions and org chart
3. **Phase 3 (Week 7+):** Integrate GraphRAG for entity queries

**When to skip GraphRAG:**
- Budget constrained (<$5k for setup)
- Need deployment in <1 week
- Queries are mostly factual lookup (not relationship-focused)

---

**Q5: Explain the difference between a Knowledge Graph and a vector database. Can you combine them? How?**

**Expected Answer:**

**Knowledge Graph:**
```
Structure: (Entity)-[Relationship]->(Entity)
Example: (Paris)-[capital_of]->(France)
Query: Graph traversal (MATCH, Cypher)
Strength: Explicit relationships, reasoning
```

**Vector Database:**
```
Structure: Document â†’ Embedding (vector)
Example: "Paris is the capital of France" â†’ [0.1, 0.3, ..., 0.8]
Query: Similarity search (cosine)
Strength: Semantic matching
```

**Combination (Best of both worlds):**

**Approach 1: KG-Enhanced Retrieval**
```python
def kg_enhanced_retrieval(query):
    # Step 1: Extract entities from query
    entities = extract_entities(query)  # "Paris", "France"
    
    # Step 2: Expand entities via KG
    related = []
    for entity in entities:
        neighbors = kg.get_neighbors(entity, hops=1)
        related.extend(neighbors)
    # Paris â†’ {France, Eiffel Tower, Seine River}
    
    # Step 3: Search vector DB with expanded entities
    query_expanded = query + " " + " ".join(related)
    results = vector_db.search(embed(query_expanded), k=10)
    
    return results
```

**Approach 2: Hybrid Scoring**
```python
def hybrid_kg_vector_search(query):
    # Vector similarity
    query_emb = embed(query)
    vector_results = vector_db.search(query_emb, k=50)
    
    # KG relevance (entity overlap)
    query_entities = extract_entities(query)
    kg_scores = {}
    for doc in vector_results:
        doc_entities = extract_entities(doc.text)
        # Graph distance between query entities and doc entities
        graph_distance = kg.shortest_path(query_entities, doc_entities)
        kg_scores[doc.id] = 1 / (1 + graph_distance)  # Closer = higher score
    
    # Combine
    final_scores = {}
    for doc in vector_results:
        final_scores[doc.id] = (
            0.7 * doc.vector_score + 
            0.3 * kg_scores.get(doc.id, 0)
        )
    
    return sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:10]
```

**Real-world Example (Weaviate + Neo4j):**
```python
# Weaviate for vector search
weaviate_results = weaviate_client.query.get("Document", ["text"]) \
    .with_near_text({"concepts": [query]}) \
    .with_limit(50) \
    .do()

# Neo4j for entity relationships
neo4j_query = """
MATCH (q_entity)-[*1..2]-(doc_entity)-[:MENTIONED_IN]->(doc)
WHERE q_entity.name IN $query_entities
RETURN doc.id, COUNT(*) as relevance
ORDER BY relevance DESC
LIMIT 50
"""
neo4j_results = neo4j_session.run(neo4j_query, query_entities=["Paris", "France"])

# Merge results (RRF or weighted combination)
final_results = merge_rankings(weaviate_results, neo4j_results)
```

**When to combine:**
- Complex domain with rich entity relationships
- Queries require both semantic matching AND reasoning
- Budget allows (maintaining both systems)

**Trade-off:**
- **Complexity:** 2 systems to maintain
- **Latency:** Two queries instead of one (50-100ms overhead)
- **Benefit:** Better accuracy (10-20% improvement on entity-heavy queries)

---

**Q6: You're building GraphRAG for a medical knowledge base. You extract entities and relationships using GPT-4. How do you validate the quality of your Knowledge Graph? What are the failure modes?**

**Expected Answer:**

**Validation Strategy:**

**1. Entity Extraction Validation**
```python
# Sample-based validation
sample_docs = random.sample(documents, 100)

for doc in sample_docs:
    extracted_entities = gpt4_extract_entities(doc)
    
    # Manual review (gold standard)
    gold_entities = human_annotator.annotate(doc)
    
    # Metrics
    precision = len(extracted âˆ© gold) / len(extracted)
    recall = len(extracted âˆ© gold) / len(gold)
    f1 = 2 * (precision * recall) / (precision + recall)
```

**Typical results:**
- **Precision:** 85-95% (some false positives)
- **Recall:** 70-85% (misses some entities)
- **Medical domain:** Often lower (specialized terminology)

**2. Relationship Extraction Validation**
```python
# More challenging than entities
for doc in sample_docs:
    extracted_rels = gpt4_extract_relations(doc)
    gold_rels = human_annotator.annotate_relations(doc)
    
    # Exact match
    exact_precision = len(extracted âˆ© gold) / len(extracted)
    
    # Relaxed match (same entities, different relation name)
    relaxed_precision = len(same_entities(extracted, gold)) / len(extracted)
```

**Typical results:**
- **Exact match:** 60-75% (relation names vary)
- **Relaxed match:** 80-90%

**3. Entity Resolution Validation**
```python
# Problem: "COVID-19" = "Covid" = "SARS-CoV-2"?
# Check for duplicates
entities = kg.get_all_entities()
potential_duplicates = find_similar_names(entities, threshold=0.8)

for e1, e2 in potential_duplicates:
    if human_annotator.confirm_duplicate(e1, e2):
        kg.merge_entities(e1, e2)
```

**4. Graph Structure Validation**
```python
# Sanity checks
def validate_kg_structure(kg):
    issues = []
    
    # Check 1: Isolated nodes (no relationships)
    isolated = kg.find_isolated_nodes()
    if len(isolated) > 0.1 * kg.num_nodes():
        issues.append(f"Too many isolated nodes: {len(isolated)}")
    
    # Check 2: Orphan relationships (entity doesn't exist)
    orphans = kg.find_orphan_relationships()
    if len(orphans) > 0:
        issues.append(f"Orphan relationships: {len(orphans)}")
    
    # Check 3: Highly connected nodes (>100 edges)
    hubs = kg.find_hubs(degree_threshold=100)
    # Manual review: Are these legitimate hubs or extraction errors?
    
    # Check 4: Duplicate relationships
    duplicates = kg.find_duplicate_edges()
    if len(duplicates) > 0:
        issues.append(f"Duplicate edges: {len(duplicates)}")
    
    return issues
```

**Failure Modes:**

**1. Entity Extraction Failures**
- **Abbreviation ambiguity:** "MS" = Multiple Sclerosis or Mass Spectrometry?
  - **Fix:** Context-aware extraction, domain glossary
  
- **Nested entities:** "Non-small cell lung cancer" vs "lung cancer"
  - **Fix:** Entity hierarchy, keep both with parent-child relationship

- **Negation blindness:** "No evidence of diabetes" â†’ Incorrectly extracts "diabetes"
  - **Fix:** Negation detection in prompt

**2. Relationship Extraction Failures**
- **Implicit relationships:** "Aspirin reduces pain" â†’ (Aspirin, TREATS, Pain)?
  - **Fix:** Relationship normalization, ontology mapping

- **Temporal relationships:** "Drug X was approved in 2020, recalled in 2021"
  - **Fix:** Add temporal attributes to edges

- **Probabilistic relationships:** "May cause", "possibly related"
  - **Fix:** Add confidence scores to edges

**3. Entity Resolution Failures**
- **False merges:** "John Smith" (patient) merged with "John Smith" (doctor)
  - **Fix:** Context-based disambiguation (role, location)

- **False separates:** "COVID-19" and "Covid" not merged
  - **Fix:** Synonym dictionary, embedding-based similarity

**4. Scale Issues**
- **Extraction cost:** 10,000 docs Ã— $0.01/doc = $100 (GPT-4)
  - **Fix:** Use cheaper model (GPT-4o-mini) or open-source (Llama 3.1)

- **Inconsistency over time:** Early extractions differ from later ones
  - **Fix:** Batch process all docs with same prompt version

**Quality Improvement Loop:**
```python
# Iterative improvement
for iteration in range(5):
    # 1. Extract with current prompt
    kg = build_kg(documents, current_prompt)
    
    # 2. Validate on sample
    errors = validate_on_sample(kg, sample_size=100)
    
    # 3. Analyze error patterns
    error_types = categorize_errors(errors)
    
    # 4. Update prompt to fix top errors
    current_prompt = improve_prompt(current_prompt, error_types)
    
    # 5. Measure improvement
    new_f1 = evaluate_f1(kg)
    print(f"Iteration {iteration}: F1 = {new_f1}")
```

**Recommended Thresholds (Medical Domain):**
- Entity extraction F1: >80% (otherwise, not reliable)
- Relationship extraction F1: >70% (harder task)
- Entity resolution accuracy: >90% (critical for correctness)
- Manual review: 5-10% of extractions

---

### Advanced RAG Patterns

**Q7: Explain Self-RAG. When would it reduce costs compared to standard RAG? When would it increase costs?**

**Expected Answer:**

**Self-RAG Recap:**
```python
def self_rag(query):
    # Step 1: Try to answer without retrieval
    initial_answer = llm(query)
    
    # Step 2: Self-critique
    confidence = llm(f"How confident are you in this answer? [High/Medium/Low]\nAnswer: {initial_answer}")
    
    if confidence in ["High", "Medium"]:
        return initial_answer  # Skip retrieval
    
    # Step 3: Retrieve and regenerate
    docs = retrieve(query)
    final_answer = llm(f"Context: {docs}\nQuery: {query}")
    
    return final_answer
```

**Cost Analysis:**

**Scenario 1: Self-RAG REDUCES costs**
- **Query type:** General knowledge questions
- **Example:** "What is the capital of France?"
- **Standard RAG:** Embedding (1k tokens) + Retrieval + LLM (2k tokens) = $0.06
- **Self-RAG:** 
  - Initial answer: LLM (50 tokens) = $0.002
  - Self-critique: LLM (100 tokens) = $0.004
  - Confidence: High â†’ Skip retrieval
  - **Total: $0.006** (10x cheaper!)

**Success rate:** 30-50% of queries can skip retrieval (if LLM has knowledge)

**Scenario 2: Self-RAG INCREASES costs**
- **Query type:** Specific factual questions requiring retrieval
- **Example:** "What was Q3 revenue for Company X?"
- **Standard RAG:** 
  - Embedding + Retrieval + LLM = $0.06
- **Self-RAG:**
  - Initial answer: $0.002
  - Self-critique: $0.004
  - Confidence: Low â†’ Retrieve
  - Regenerate: $0.06
  - Verification: $0.01 (check if grounded)
  - **Total: $0.076** (27% more expensive)

**When Self-RAG is worth it:**

**Reduce costs:**
- **Query mix:** 40%+ general knowledge, 60% specific
- **User behavior:** Many questions LLM can answer (FAQs)
- **Savings:** 20-30% cost reduction

**Example calculation:**
```
1000 queries/day
- 400 general (skip retrieval): 400 Ã— $0.006 = $2.40
- 600 specific (full RAG): 600 Ã— $0.076 = $45.60
Total: $48/day = $1,440/month

Standard RAG: 1000 Ã— $0.06 = $60/day = $1,800/month

Savings: $360/month (20%)
```

**Increase costs:**
- **Query mix:** 90%+ require retrieval (specialized domain)
- **High stakes:** Need verification even if confident (adds cost)
- **Overhead:** Self-critique adds 10-30% cost with no benefit

**Optimization:**
```python
# Hybrid: Use self-RAG only for certain query types
def optimized_rag(query):
    query_type = classify_query(query)
    
    if query_type == "general_knowledge":
        return self_rag(query)  # Try to skip retrieval
    else:  # specific_factual, domain_specific
        return standard_rag(query)  # Always retrieve
```

**Trade-off summary:**
- **Potential savings:** 20-40% on mixed workloads
- **Risk:** 10-30% cost increase if most queries need retrieval
- **Complexity:** Additional self-critique logic
- **Recommendation:** A/B test on your query distribution

---

**Q8: You're building a RAG system. A user query is: "What are the main themes in our company's last 100 all-hands meetings?" Would you use vector RAG or GraphRAG? Walk through your solution.**

**Expected Answer:**

**Query Analysis:**
- **Scope:** 100 documents (large)
- **Task:** Summarization across documents (global understanding)
- **Challenge:** Standard RAG retrieves chunks, can't see big picture

**Solution: GraphRAG (Community-based summarization)**

**Why GraphRAG?**
1. **Global query:** Requires understanding across ALL documents
2. **Themes:** Need to cluster related topics
3. **Standard RAG fails:** Would retrieve random chunks, miss overall themes

**Implementation:**

**Phase 1: Build Knowledge Graph**
```python
# Extract entities from 100 transcripts
entities = []
for transcript in transcripts:
    extracted = llm_extract_entities(transcript)
    # Entities: Projects, initiatives, people, topics
    entities.extend(extracted)

# Build relationships
# Co-mention: If "Project X" and "Team Y" appear in same meeting â†’ edge
graph = build_graph(entities, relationships)
```

**Phase 2: Community Detection**
```python
import networkx as nx
from cdlib import algorithms

# Detect communities (clusters of related entities)
G = nx.Graph()
G.add_nodes_from(entities)
G.add_edges_from(relationships)

communities = algorithms.leiden(G)
# Result: 10-20 communities (themes)

# Example communities:
# Community 1: {Product Launch, Marketing, Q4 Goals}
# Community 2: {Remote Work, Office Reopening, Hybrid Policy}
# Community 3: {AI Strategy, ML Team, Data Infrastructure}
```

**Phase 3: Community Summarization**
```python
summaries = []
for community in communities:
    # Get all documents mentioning entities in this community
    relevant_docs = get_docs_for_entities(community.nodes)
    
    # Summarize
    summary = llm(f"""
        Summarize the main theme across these meeting excerpts:
        {relevant_docs}
        
        Focus on:
        - What was discussed
        - Key decisions
        - Action items
    """)
    
    summaries.append({
        "theme": community.name,
        "summary": summary,
        "supporting_docs": relevant_docs[:5]  # Top 5 for citations
    })
```

**Phase 4: Generate Final Answer**
```python
final_answer = llm(f"""
    Based on these themes from 100 all-hands meetings:
    {summaries}
    
    Provide a comprehensive overview of main themes, ranked by importance.
""")
```

**Alternative: Hierarchical Summarization (if no GraphRAG)**
```python
# Map-Reduce approach
def hierarchical_summarization(documents):
    # Step 1: Summarize each document
    doc_summaries = [llm(f"Summarize: {doc}") for doc in documents]
    
    # Step 2: Group summaries (10 at a time)
    groups = chunk_list(doc_summaries, size=10)
    group_summaries = [llm(f"Summarize: {group}") for group in groups]
    
    # Step 3: Final summary
    final = llm(f"Synthesize main themes: {group_summaries}")
    
    return final
```

**Comparison:**

| Approach | Pros | Cons | Cost | Quality |
|----------|------|------|------|---------|
| **GraphRAG** | Identifies themes automatically | Setup: 2-3 days | High ($200-500) | Excellent |
| **Hierarchical** | Simple, no KG needed | Loses nuance in merging | Medium ($50-100) | Good |
| **Standard RAG** | Fast | Can't see global picture | Low ($10) | Poor |

**Recommendation:**
- **One-time query:** Hierarchical summarization (faster setup)
- **Recurring queries:** GraphRAG (amortize setup cost)
- **Budget <$100:** Hierarchical
- **Need best quality:** GraphRAG

**Follow-up:** "The CEO wants this monthly. How do you optimize?"

**Answer:**
1. **Incremental KG updates:** Don't rebuild from scratch
   ```python
   # Add only new meetings to existing KG
   new_entities = extract_entities(new_meetings)
   graph.add_nodes(new_entities)
   recompute_communities()  # Faster than full rebuild
   ```

2. **Cache community summaries:** Update only affected communities

3. **Batch processing:** Run overnight, not real-time

4. **Cost:** Initial $500 setup, then $50/month for updates (much cheaper)

---

**Q9: Compare hybrid search (vector + BM25) to pure vector search. Give specific examples where each wins.**

**Expected Answer:**

**Pure Vector Search Wins:**

**Example 1: Paraphrase queries**
```
Query: "How can I improve my model's accuracy?"
Document: "Techniques for enhancing neural network performance"

BM25: Low score (no keyword overlap)
Vector: High score (semantic match)
```

**Example 2: Multilingual search**
```
Query: "machine learning" (English)
Document: "apprentissage automatique" (French)

BM25: 0 score (different languages)
Vector: High score (same concept, multilingual embeddings)
```

**Example 3: Conceptual queries**
```
Query: "Ways to reduce carbon emissions"
Document: "Electric vehicles lower CO2 output by 60%"

BM25: Low (no overlap with "carbon emissions")
Vector: High ("CO2 output" semantically related)
```

**Hybrid Search (Vector + BM25) Wins:**

**Example 1: Exact names/IDs**
```
Query: "Error code E-4532"
Document: "To resolve E-4532, restart the service."

BM25: Perfect match (exact string)
Vector: Might retrieve E-4531 or similar (too fuzzy)

Hybrid: Best of both (BM25 ensures exact match)
```

**Example 2: Rare technical terms**
```
Query: "Kubernetes HorizontalPodAutoscaler"
Document: "Configure HPA (HorizontalPodAutoscaler) for scaling"

BM25: High score (rare term â†’ high IDF)
Vector: May conflate with general "autoscaling" docs

Hybrid: BM25 boosts exact technical term, vector adds context
```

**Example 3: Acronyms**
```
Query: "BERT fine-tuning"
Document A: "Bidirectional Encoder Representations from Transformers"
Document B: "BERT can be fine-tuned for downstream tasks"

Vector: Retrieves both (semantically related)
BM25: Retrieves only B (exact "BERT")

Hybrid: Ranks B higher (exact match + semantic), but includes A
```

**Pure BM25 Wins (Rare):**

**Example: Boolean logic queries**
```
Query: "machine learning" NOT "deep learning"

BM25: Can implement boolean logic
Vector: No concept of negation

Use case: Legal search, advanced filtering
```

**Quantitative Comparison (Typical):**

```
Dataset: 10,000 technical documents
Queries: 500 test queries

Pure Vector (text-embedding-3-large):
- MRR: 0.72
- NDCG@10: 0.68
- Latency: 50ms

Pure BM25:
- MRR: 0.65
- NDCG@10: 0.61
- Latency: 30ms

Hybrid (Î±=0.7):
- MRR: 0.79 (+10% over vector)
- NDCG@10: 0.75 (+10%)
- Latency: 60ms
```

**Alpha (Î±) tuning:**
```python
# Î± = weight for vector search (0-1)
# 1-Î± = weight for BM25

results = {}
for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:
    scores = evaluate_on_test_set(alpha)
    results[alpha] = scores

# Typical findings:
# Î±=0.0 (pure BM25): Good for exact matches
# Î±=0.5 (balanced): Good for mixed queries
# Î±=0.7-0.8 (mostly vector): Best for most datasets
# Î±=1.0 (pure vector): Good for conceptual queries
```

**Decision framework:**

```
Query characteristics:
â”œâ”€ Technical terms, IDs, codes â†’ Lower Î± (more BM25)
â”œâ”€ Conceptual, paraphrased â†’ Higher Î± (more vector)
â”œâ”€ Mixed â†’ Î±=0.7 (default)
â””â”€ Multilingual â†’ Î±=1.0 (vector only)

Dataset characteristics:
â”œâ”€ Well-structured, consistent terminology â†’ BM25 works well
â”œâ”€ Natural language, varied phrasing â†’ Vector better
â””â”€ Both â†’ Hybrid (Î±=0.7)
```

**Implementation tip:**
```python
# Adaptive alpha based on query analysis
def adaptive_alpha(query):
    if has_quoted_strings(query):  # "exact phrase"
        return 0.3  # Prioritize BM25
    elif has_special_chars(query):  # E-4532, UUID
        return 0.2
    elif is_short(query):  # 1-2 words
        return 0.5  # Balanced
    else:
        return 0.75  # Default: mostly vector
```

---

### System Design & Production

**Q10: You're deploying a RAG system to production. It needs to handle 10,000 queries per day with <200ms P95 latency. Walk through your architecture, including specific technologies and cost estimates.**

**Expected Answer:**

**Architecture:**

```
User Query
    â†“
API Gateway (FastAPI)
    â†“
Load Balancer
    â†“
    â”œâ”€â†’ Query Processing Service (3 instances)
    â”‚       â†“
    â”‚   Embedding (text-embedding-3-large)
    â”‚       â†“
    â”œâ”€â†’ Vector DB (Qdrant cluster: 3 nodes)
    â”‚   
    â”œâ”€â†’ Reranker Service (Optional, 2 instances)
    â”‚
    â””â”€â†’ LLM Service (GPT-4o-mini)
            â†“
        Response
```

**Component Details:**

**1. API Gateway (FastAPI)**
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import asyncio

app = FastAPI()

@app.post("/query")
async def query(request: QueryRequest):
    # Async pipeline
    embedding_task = asyncio.create_task(get_embedding(request.query))
    
    embedding = await embedding_task
    results = await vector_search(embedding)
    
    # Optional: Rerank
    if request.use_reranker:
        results = await rerank(request.query, results)
    
    answer = await llm_generate(request.query, results)
    
    return {"answer": answer, "sources": results}
```

**Infrastructure:**
- **Instances:** 3 Ã— t3.medium (AWS)
- **Auto-scaling:** Scale to 10 during peak
- **Cost:** $50/month (base) + $30/month (peak hours)

**2. Embedding Service**
```python
import openai

async def get_embedding(text):
    response = await openai.Embedding.acreate(
        model="text-embedding-3-large",
        input=text
    )
    return response['data'][0]['embedding']
```

**Cost calculation:**
```
10,000 queries/day Ã— 30 days = 300,000 queries/month
Average query: 50 tokens
Total: 15M tokens/month

OpenAI embedding cost: $0.13 per 1M tokens
Cost: 15 Ã— $0.13 = $1.95/month
```

**3. Vector Database (Qdrant)**
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

client = QdrantClient(host="qdrant-cluster", port=6333)

# Collection setup
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=3072,  # text-embedding-3-large
        distance=Distance.COSINE
    )
)

# Search
results = client.search(
    collection_name="documents",
    query_vector=embedding,
    limit=10,
    query_filter={"date": {"$gte": "2024-01-01"}}  # Optional filter
)
```

**Infrastructure:**
- **Cluster:** 3 Ã— c5.2xlarge (AWS)
  - 8 vCPU, 16 GB RAM each
  - SSD storage: 500 GB per node
- **Documents:** 100,000 documents Ã— 3KB avg = 300 MB
- **Vectors:** 100,000 Ã— 3072 Ã— 4 bytes (float32) = 1.2 GB
- **Total storage:** ~2 GB (with HNSW index overhead)
- **Cost:** $400/month (3 nodes Ã— $133/month)

**Alternative (cheaper):**
- **Managed Qdrant Cloud:** $99/month (1 node, suitable for 100K vectors)
- **Self-hosted on smaller instances:** $150/month (2 Ã— t3.large)

**4. Reranker Service (Optional)**
```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

async def rerank(query, documents, top_k=10):
    pairs = [[query, doc.text] for doc in documents]
    scores = reranker.predict(pairs)
    
    reranked = sorted(zip(documents, scores), 
                     key=lambda x: x[1], 
                     reverse=True)
    
    return [doc for doc, _ in reranked[:top_k]]
```

**Infrastructure:**
- **Instances:** 2 Ã— g4dn.xlarge (GPU: NVIDIA T4)
- **Model:** Loaded in GPU memory (500 MB)
- **Throughput:** 100 reranks/sec per instance
- **Cost:** $400/month (2 Ã— $200/month)
- **Decision:** Skip if budget constrained (save $400/month, -5% accuracy)

**5. LLM Service (GPT-4o-mini)**
```python
async def llm_generate(query, context):
    prompt = f"""
    Context: {format_context(context)}
    
    Question: {query}
    
    Answer based on the context above. If the context doesn't contain 
    the answer, say "I don't have enough information."
    """
    
    response = await openai.ChatCompletion.acreate(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.3
    )
    
    return response.choices[0].message.content
```

**Cost calculation:**
```
10,000 queries/day Ã— 30 days = 300,000 queries/month

Input tokens per query:
- Context: 10 chunks Ã— 200 tokens = 2,000
- Query: 50 tokens
- Total input: 2,050 tokens

Output tokens: 500 tokens avg

Total tokens/month:
- Input: 300,000 Ã— 2,050 = 615M tokens
- Output: 300,000 Ã— 500 = 150M tokens

GPT-4o-mini pricing:
- Input: $0.150 per 1M tokens
- Output: $0.600 per 1M tokens

Cost:
- Input: 615 Ã— $0.150 = $92.25
- Output: 150 Ã— $0.600 = $90.00
- Total: $182.25/month
```

**6. Caching Layer (Redis)**
```python
import redis
import hashlib

redis_client = redis.Redis(host='localhost', port=6379)

async def cached_query(query):
    # Cache key: hash of query
    cache_key = hashlib.md5(query.encode()).hexdigest()
    
    # Check cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Compute
    result = await process_query(query)
    
    # Cache for 1 hour
    redis_client.setex(cache_key, 3600, json.dumps(result))
    
    return result
```

**Impact:**
- **Cache hit rate:** 20-30% (common queries)
- **Savings:** 25% reduction in LLM calls (~$45/month)
- **Infrastructure:** t3.medium (Redis) = $30/month
- **Net savings:** $15/month (not huge, but helps with latency)

**7. Monitoring (Prometheus + Grafana)**
```python
from prometheus_client import Counter, Histogram

query_counter = Counter('rag_queries_total', 'Total queries')
latency_histogram = Histogram('rag_query_latency_seconds', 'Query latency')

@app.post("/query")
@latency_histogram.time()
async def query(request: QueryRequest):
    query_counter.inc()
    # ... process query
```

**Dashboards:**
- Queries per second (QPS)
- P50, P95, P99 latency
- Error rate
- Cache hit rate
- LLM token usage

**Cost:** $15/month (t3.small for Prometheus + Grafana)

**Total Cost Breakdown:**

| Component | Monthly Cost | Notes |
|-----------|-------------|-------|
| API Gateway (3 instances) | $80 | t3.medium Ã— 3 |
| Embedding API (OpenAI) | $2 | text-embedding-3-large |
| Vector DB (Qdrant Cloud) | $99 | Managed, 1 node |
| Reranker (Optional) | $0 | Skipped to save cost |
| LLM API (GPT-4o-mini) | $182 | 300K queries/month |
| Caching (Redis) | $30 | t3.medium |
| Monitoring | $15 | Prometheus + Grafana |
| **Total** | **$408/month** | Without reranker |

**With reranker:** $808/month (+$400)

**Latency Breakdown (P95):**

```
1. API overhead: 5ms
2. Embedding: 20ms (OpenAI API)
3. Vector search: 30ms (Qdrant)
4. Reranker: 0ms (skipped)
5. LLM generation: 800ms (GPT-4o-mini)
6. Total: 855ms

Problem: Exceeds 200ms requirement!
```

**Optimization for <200ms latency:**

**Option 1: Streaming response**
```python
@app.post("/query")
async def query_stream(request: QueryRequest):
    # Return immediately with sources
    results = await vector_search(embedding)
    
    # Stream LLM response
    async def generate():
        async for chunk in llm_stream(request.query, results):
            yield chunk
    
    return StreamingResponse(generate())
```
- **Perceived latency:** 50ms (return sources immediately)
- **Full response:** 800ms (user sees progressive output)

**Option 2: Faster LLM**
- **Switch to:** GPT-4o-mini (same model, different endpoint config)
- **Latency:** 300-500ms (vs 800ms)
- **Trade-off:** Slightly shorter responses

**Option 3: Smaller context**
```python
# Reduce chunks: 10 â†’ 5
results = vector_search(embedding, limit=5)
# Latency: 400ms (less input tokens)
```

**Option 4: Precompute common queries**
```python
# Batch process top 100 queries nightly
common_queries = get_top_queries()
for query in common_queries:
    answer = await process_query(query)
    redis_client.set(query_hash, answer, ex=86400)  # 24hr cache
```

**Revised latency (with optimizations):**
```
1. API overhead: 5ms
2. Embedding: 20ms
3. Vector search (5 chunks): 25ms
4. LLM (streaming): 50ms (time to first token)
5. Total: 100ms âœ“

Full response: 400ms (user sees progressive output)
```

**Scaling:**

**10x traffic (100,000 queries/day):**
- **API Gateway:** Scale to 30 instances (+$800/month)
- **Vector DB:** Scale to 3-node cluster (+$200/month)
- **LLM costs:** 10x increase (+$1,820/month)
- **Total:** ~$3,000/month

**100x traffic (1M queries/day):**
- **Architecture change:** Add CDN, edge caching
- **LLM:** Consider self-hosted (Llama 3.1) on GPU instances
- **Cost:** $15,000-20,000/month (vs $180K with pure API calls)

---

**Q11: Your RAG system's accuracy drops from 85% to 70% over 3 months. What are the potential causes and how do you debug?**

**Expected Answer:**

**Potential Causes:**

**1. Data Drift**
```python
# Symptom: New documents have different style/structure

# Debug:
from scipy.stats import ks_2samp

old_embeddings = get_embeddings(old_documents)
new_embeddings = get_embeddings(new_documents)

# KS test: Are distributions different?
statistic, p_value = ks_2samp(
    old_embeddings.flatten(),
    new_embeddings.flatten()
)

if p_value < 0.05:
    print("Significant drift detected!")
```

**Example:** Company switches from formal reports to casual Slack messages
- **Impact:** Embedding model trained on formal text struggles
- **Fix:** Fine-tune embedding model on new data, or add domain adaptation layer

**2. Query Distribution Shift**
```python
# Debug: Compare old vs new query patterns
old_queries = load_queries("2024-01-01", "2024-03-01")
new_queries = load_queries("2024-03-01", "2024-06-01")

# Cluster queries
from sklearn.cluster import KMeans

old_clusters = KMeans(n_clusters=10).fit(embed_queries(old_queries))
new_clusters = KMeans(n_clusters=10).fit(embed_queries(new_queries))

# Compare cluster distributions
# If new clusters emerge, users asking different questions
```

**Example:** Users start asking about new product line (not in docs)
- **Impact:** Retrieval finds irrelevant docs
- **Fix:** Add new product docs, update knowledge base

**3. Document Staleness**
```python
# Debug: Check document ages
doc_ages = [doc.timestamp for doc in documents]
avg_age = mean(doc_ages)

if avg_age > 180:  # 6 months
    print("Documents getting stale!")
```

**Example:** Docs from 2023, users ask about 2024 policies
- **Impact:** Retrieves outdated info
- **Fix:** Refresh documents, remove old ones, add recency weighting

**4. Embedding Model Degradation**
```python
# Debug: Test on fixed benchmark
benchmark_queries = load_benchmark()  # Fixed test set

accuracy_over_time = []
for month in range(6):
    accuracy = evaluate_on_benchmark(benchmark_queries, month)
    accuracy_over_time.append(accuracy)

if accuracy_over_time is declining:
    print("Model performance degrading!")
```

**Possible causes:**
- Embedding API changed (OpenAI updated model)
- Vector DB index needs rebuilding
- Quantization errors accumulated

**Fix:**
- Re-embed all documents with current model
- Rebuild index
- Verify embedding API version

**5. Retrieval Pipeline Issues**
```python
# Debug: Measure retrieval metrics over time
metrics = {
    "precision@10": [],
    "recall@10": [],
    "ndcg@10": []
}

for month in months:
    sample_queries = get_queries(month)
    precision = calculate_precision(sample_queries)
    metrics["precision@10"].append(precision)

if precision is declining:
    print("Retrieval quality degrading!")
```

**Specific checks:**
- **Index corruption:** Rebuild vector index
- **Sharding issues:** Check if documents distributed unevenly
- **Filter errors:** Are metadata filters working correctly?

**6. LLM Changes**
```python
# Debug: LLM might have updated (API-based)

# Check model version
current_version = openai.model_info("gpt-4o-mini")

# Compare outputs on same inputs
for query in test_queries:
    old_answer = get_cached_answer(query, "2024-01-01")
    new_answer = generate_answer(query, "2024-06-01")
    
    if jaccard_similarity(old_answer, new_answer) < 0.5:
        print(f"LLM behavior changed for: {query}")
```

**Example:** OpenAI updated GPT-4o-mini, changed instruction following
- **Impact:** Different answer format, breaks downstream parsing
- **Fix:** Update prompt, handle new format

**Debugging Workflow:**

**Step 1: Isolate the failure point**
```python
def debug_rag_pipeline(query):
    print("=== Debugging RAG Pipeline ===")
    
    # 1. Embedding
    embedding = embed(query)
    print(f"Embedding shape: {embedding.shape}")
    
    # 2. Retrieval
    results = vector_search(embedding, k=10)
    print(f"Retrieved {len(results)} documents")
    
    # Manual review: Are these relevant?
    for i, doc in enumerate(results):
        print(f"{i+1}. {doc.text[:100]}... (score: {doc.score})")
    
    # 3. LLM generation
    answer = llm(query, results)
    print(f"Answer: {answer}")
    
    # 4. Ground truth comparison
    expected = get_ground_truth(query)
    accuracy = compare(answer, expected)
    print(f"Accuracy: {accuracy}")
    
    return {
        "embedding_ok": embedding is not None,
        "retrieval_ok": len(results) > 0,
        "answer_ok": accuracy > 0.7
    }
```

**Step 2: A/B test fixes**
```python
# Compare old vs new configuration
old_config = load_config("2024-01-01")
new_config = current_config()

# Run evaluation on both
old_accuracy = evaluate(old_config, test_set)
new_accuracy = evaluate(new_config, test_set)

print(f"Old accuracy: {old_accuracy}")
print(f"New accuracy: {new_accuracy}")

# Identify which component changed
if old_config.embedding_model != new_config.embedding_model:
    print("Embedding model changed!")
```

**Step 3: Root cause analysis**

Common culprits (ordered by frequency):
1. **Document staleness (40%):** Docs haven't been updated
2. **Query shift (25%):** Users asking different questions
3. **Data drift (20%):** New document style
4. **LLM changes (10%):** API provider updated model
5. **Infrastructure (5%):** Index corruption, config errors

**Fixes:**

**Quick wins (1 week):**
```python
# 1. Refresh documents
ingest_new_documents()
remove_old_documents(older_than="6 months")

# 2. Rebuild index
vector_db.rebuild_index()

# 3. Update prompts
update_system_prompt("""
    [New instructions based on query analysis]
""")
```

**Medium-term (1 month):**
```python
# 1. Fine-tune embedding model on current data
from sentence_transformers import SentenceTransformer, InputExample, losses

model = SentenceTransformer('all-MiniLM-L6-v2')

train_examples = [
    InputExample(texts=[query, positive_doc, negative_doc])
    for query, pos, neg in training_data
]

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.TripletLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3
)
```

**Long-term (3 months):**
```python
# 1. Implement monitoring and alerting
def setup_monitoring():
    # Track accuracy over time
    schedule.every().day.at("00:00").do(evaluate_accuracy)
    
    # Alert if drops > 5%
    if accuracy_drop > 0.05:
        send_alert("RAG accuracy dropped!")
    
    # Track query distribution
    monitor_query_clusters()
    
    # Track document freshness
    alert_if_docs_older_than(6 months)

# 2. A/B testing infrastructure
def ab_test_changes():
    # Route 10% traffic to new config
    if random() < 0.1:
        return new_pipeline(query)
    else:
        return old_pipeline(query)
```

**Prevention:**
```python
# Continuous monitoring
def rag_health_check():
    checks = {
        "document_freshness": check_doc_freshness(),
        "retrieval_quality": evaluate_retrieval(sample_queries),
        "llm_quality": evaluate_llm(sample_queries),
        "latency": measure_latency(),
        "cost": measure_cost()
    }
    
    for check, status in checks.items():
        if status.is_failing():
            alert(f"{check} is failing!")
    
    return checks
```

---

### Cost & Trade-off Questions

**Q12: You need to reduce RAG costs by 50% while maintaining >80% of current accuracy. What levers do you pull? Quantify the trade-offs.**

**Expected Answer:**

**Current Baseline:**
```
10,000 queries/day
- Embedding (text-embedding-3-large): $2/month
- Vector DB (Qdrant): $99/month
- LLM (GPT-4): $1,500/month (input: $1.2M tokens, output: $300K tokens)
- Total: $1,601/month
```

**Target: $800/month (50% reduction)**

**Levers:**

**Lever 1: Switch from GPT-4 to GPT-4o-mini**
```
Current (GPT-4):
- Input: $10 per 1M tokens
- Output: $30 per 1M tokens
- Cost: (1.2M Ã— $10) + (0.3M Ã— $30) = $21/day = $630/month

New (GPT-4o-mini):
- Input: $0.150 per 1M tokens
- Output: $0.600 per 1M tokens
- Cost: (1.2M Ã— $0.150) + (0.3M Ã— $0.600) = $0.36/day = $10.8/month

Savings: $630 - $11 = $619/month (41% of target)
Accuracy impact: -3 to -5% (based on benchmarks)
```

**Lever 2: Reduce context size (10 chunks â†’ 5 chunks)**
```
Current: 10 chunks Ã— 200 tokens = 2,000 tokens
New: 5 chunks Ã— 200 tokens = 1,000 tokens

Input tokens reduced by 50%
LLM cost: $11 â†’ $6/month

Savings: $5/month (small, but also improves latency)
Accuracy impact: -2 to -4% (fewer context, may miss relevant info)
```

**Lever 3: Aggressive caching**
```python
# Current cache hit rate: 20%
# Target cache hit rate: 50% (aggressive)

# Strategies:
# 1. Semantic caching (similar queries)
def semantic_cache(query):
    # Find similar queries in cache
    similar = find_similar_queries(query, threshold=0.9)
    if similar:
        return cached_answer(similar)
    return None

# 2. Precompute top 500 queries
most_common = get_top_queries(n=500)
for query in most_common:
    cache[query] = generate_answer(query)

# Impact:
# 50% cache hit rate = 50% fewer LLM calls
# Savings: $6 Ã— 0.5 = $3/month
```

**Total savings so far: $619 + $5 + $3 = $627/month**
**New cost: $1,601 - $627 = $974/month**

**Still need: $974 - $800 = $174 more savings**

**Lever 4: Switch to self-hosted embedding**
```
Current: text-embedding-3-large (OpenAI) = $2/month
New: Self-hosted BGE-large = $0/month (+ $50/month for GPU instance)

Wait, this increases cost! Skip.
```

**Lever 5: Cheaper vector DB**
```
Current: Qdrant Cloud = $99/month
New: Self-hosted Qdrant on t3.large = $50/month

Savings: $49/month
Trade-off: Need to manage instance, backups, updates
```

**Lever 6: Batch processing for non-urgent queries**
```python
# Identify non-urgent queries (e.g., analytics reports)
# Process in batches overnight

urgent = []
non_urgent = []

for query in queries:
    if query.priority == "high":
        urgent.append(query)
    else:
        non_urgent.append(query)

# Process urgent: Real-time
# Process non-urgent: Batch (1000 at a time, better LLM utilization)

# Savings: 10-20% cost reduction through batching
# = $6 Ã— 0.15 = $1/month (marginal)
```

**Lever 7: Prompt compression**
```python
# Use LLMLingua to compress prompts
from llmlingua import PromptCompressor

compressor = PromptCompressor()

# Original prompt: 2,000 tokens
# Compressed: 1,000 tokens (50% reduction)

compressed = compressor.compress_prompt(
    context=context,
    question=question,
    target_token=1000
)

# Savings: 50% fewer input tokens
# = $6 Ã— 0.5 = $3/month
```

**Final Cost Breakdown:**

| Change | Savings | Accuracy Impact | Complexity |
|--------|---------|----------------|------------|
| GPT-4 â†’ GPT-4o-mini | $619/month | -3 to -5% | Low |
| 10 chunks â†’ 5 chunks | $5/month | -2 to -4% | Low |
| Aggressive caching (50% hit) | $3/month | 0% | Medium |
| Self-hosted vector DB | $49/month | 0% | High |
| Prompt compression | $3/month | -1 to -2% | Medium |
| **Total savings** | **$679/month** | **-6 to -11%** | |
| **New cost** | **$922/month** | | |

**Oops, still $122 over target!**

**Lever 8 (aggressive): Hybrid LLM strategy**
```python
# Use GPT-4o-mini for 80% of queries
# Use GPT-4 only for complex queries (20%)

def select_llm(query):
    complexity = estimate_complexity(query)
    
    if complexity > 0.8:
        return "gpt-4"  # Complex
    else:
        return "gpt-4o-mini"  # Simple

# Cost:
# 80% Ã— $11/month (GPT-4o-mini) = $8.80
# 20% Ã— $630/month (GPT-4) = $126
# Total: $135/month

# Savings vs all GPT-4: $630 - $135 = $495/month
# But we already switched to GPT-4o-mini, so:
# Additional savings vs all GPT-4o-mini: $0 (no benefit)
```

**Lever 9 (nuclear option): Self-hosted LLM**
```
Infrastructure:
- 1 Ã— A100 GPU instance (AWS p4d.24xlarge) = $32/hour
- Run 2 hours/day (batch process): $64/day = $1,920/month

Wait, more expensive! But...

Alternative: Modal, Replicate (serverless)
- Llama 3.1 8B: $0.10 per 1M tokens (vs $0.15 for GPT-4o-mini)
- Savings: 33%
- $11 Ã— 0.67 = $7.37/month

Not enough savings for the complexity.
```

**Final Recommendation:**

**Achieve $800/month target:**
1. âœ… GPT-4 â†’ GPT-4o-mini: -$619/month, -5% accuracy
2. âœ… 10 chunks â†’ 5 chunks: -$5/month, -3% accuracy
3. âœ… Aggressive caching (50%): -$3/month, -0% accuracy
4. âœ… Self-hosted vector DB: -$49/month, -0% accuracy
5. âœ… Prompt compression: -$3/month, -2% accuracy
6. âœ… Skip reranker (if using): -$0 already, -0%

**Total:**
- **New cost:** $922/month (vs $800 target)
- **Accuracy drop:** ~10% (90% to 81%)
- **Problem:** Still $122 over budget, and accuracy dropped more than 20% target

**Better strategy:**

**Prioritize accuracy (keep >80%):**
1. âœ… GPT-4 â†’ GPT-4o-mini: -$619, -5% accuracy (85% â†’ 80%)
2. âœ… Caching (50%): -$3
3. âœ… Self-hosted vector DB: -$49
4. âŒ Skip aggressive prompt compression (hurts accuracy)

**Final:**
- **Cost:** $930/month (42% reduction, not 50%)
- **Accuracy:** 80-82% (meets threshold)

**Recommendation to stakeholder:**
"We can achieve **42% cost reduction** while maintaining **80%+ accuracy**. 
To reach 50% reduction would require compromising accuracy below acceptable threshold. 
Alternative: Increase accuracy threshold tolerance to 75%, then we can hit 50% cost reduction."

---

This completes the comprehensive GraphRAG & RAG Variants guide with Hao Hoang-style interview questions!

**Total Coverage:**
- âœ… GraphRAG & Knowledge Graphs explained
- âœ… Different RAG types (Vector, Hybrid, Self-RAG, CRAG, etc.)
- âœ… Graph databases comparison
- âœ… Detailed trade-off analysis
- âœ… 12 production-grade interview questions with detailed answers
- âœ… Real-world examples from Microsoft, Meta, OpenAI, etc.
- âœ… Cost analysis and optimization strategies
